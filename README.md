
ğŸ“˜ Sign to Text â€” Real-Time Sign Language Recognition (ML + MediaPipe)
Welcome to Sign to Text, a real-time hand-sign recognition system that uses MediaPipe, Machine Learning, and Text-to-Speech to convert gestures into spoken words or full sentences.
This project is ideal for beginners and advanced learners exploring computer vision, gesture recognition, and AI-driven communication tools.

âœ¨ Features
âœ” Real-time hand detection
Uses MediaPipe Hands to track 21 3D landmarks at high FPS.
âœ” Custom ML sign classification
Train a RandomForestClassifier on your own hand gestures (YES / NO / HELLO / etc.)
âœ” Live gesture â†’ spoken word conversion
Predicted signs are converted to speech using pyttsx3.
âœ” Sentence generation
The system can build and speak multi-word sentences based on your gestures.
âœ” Easy to extend
Add new gestures simply by collecting more data and retraining.

ğŸ—‚ Project Structure
sign_to_speech/
â”‚â”€â”€ collect_data.py           # Collect YES/NO gesture data (press Y/N)
â”‚â”€â”€ train_model.py            # Train RandomForest on collected data
â”‚â”€â”€ sign_to_speech_ml.py      # Real-time recognition & speech
â”‚â”€â”€ sign_data.csv             # Auto-generated dataset
â”‚â”€â”€ model.joblib              # Trained ML model
â”‚â”€â”€ README.md                 # Project documentation
â”‚â”€â”€ requirements.txt          # Dependencies
â”‚â”€â”€ assets/
â”‚     â””â”€â”€ hello_cat.gif       # GIF used in README

ğŸ“¦ Installation

Install all required packages:

python -m pip install opencv-python mediapipe pyttsx3 scikit-learn pandas numpy joblib

Works with Python 3.9â€“3.12.

ğŸ¥ Step 1 â€” Collect Training Data

Run:

python collect_data.py


Controls:

Key	Action
Y	Save current frame as label yes
N	Save current frame as label no
Q	Quit

Important:

Record 50â€“100 samples per sign (YES & NO minimum)

Samples are appended to sign_data.csv

Ensure your hand is visible before pressing keys

ğŸ§  Step 2 â€” Train the ML Model

Train a classifier on the collected dataset:

python train_model.py


This will:

Load sign_data.csv

Normalize data using StandardScaler

Train a RandomForestClassifier

Show accuracy + classification report

Save the model bundle (model.joblib)

Example output:

Loaded 200 samples
Label distribution:
yes    100
no     100
Training model...
Accuracy: 95%
Saved trained model to model.joblib

ğŸ—£ Step 3 â€” Real-Time Sign â†’ Speech

Run:

python sign_to_speech_ml.py


This will:

Open your webcam

Detect your hand

Predict YES/NO

Speak the result aloud

Controls
Key	Action
Q	Quit
ğŸ–¼ How It Works
1. MediaPipe Landmark Extraction

MediaPipe gives 21 hand points â†’ each with (x, y, z).

Total features = 63 per frame.

2. Machine Learning Classification

A RandomForestClassifier learns patterns in these 63 features.

3. Prediction Stabilization

A short rolling window makes predictions stable (no flickering).

4. Text-to-Speech Output

Predicted word is spoken using pyttsx3.

ğŸ“ˆ Adding More Signs

You can expand the vocabulary easily:

Modify collect_data.py

Add more keys like H â†’ â€œhelloâ€, T â†’ â€œthankyouâ€

Collect new samples

Retrain:

python train_model.py


Run inference again

You can add:

HELLO

PLEASE

STOP

OK

I LOVE YOU

Custom commands (e.g., â€œOpen Browserâ€)

ğŸ§© Requirements (Python)

requirements.txt:

opencv-python
mediapipe
pyttsx3
numpy
pandas
scikit-learn
joblib


ğŸ‘¤ Author

Indrayudh Bandyopadhyay
ECE Undergrad â€¢ AI/ML Enthusiast
GitHub: @Ibaner20065
